{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d9f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ------------------------------------------------------------------\n",
    "# Using standard library facilities:\n",
    "# - os: interacting with the operating system for environment variables and file paths.\n",
    "import os\n",
    "\n",
    "# Using NumPy for efficient numerical arrays and vector operations required by embedding tensors.\n",
    "import numpy as np\n",
    "\n",
    "# Using the OpenAI SDK as the model-facing client for text generation and embeddings.\n",
    "from openai import OpenAI\n",
    "\n",
    "# Using Chroma as the vector database for persistent storage and similarity search over embeddings.\n",
    "import chromadb\n",
    "\n",
    "# Using the EmbeddingFunction protocol to adapt an embedding provider to Chroma’s ingestion/query pipeline.\n",
    "from chromadb.api.types import EmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054c76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration block ------------------------------------------------------\n",
    "# Using environment variables to externalize credentials, enabling reproducible\n",
    "# experimentation across local development, CI, and cloud deployments without hard-coded secrets.\n",
    "OPENAI_API = os.getenv(\"OPENAI_API\")\n",
    "assert OPENAI_API, \"Missing OPENAI_API\"\n",
    "\n",
    "# Using explicit model identifiers to keep ablations and benchmarking reproducible.\n",
    "# LLM_MODEL targets the conversational model for generation; EMB_MODEL targets the embedding model for vectorization.\n",
    "LLM_MODEL = \"gpt-4.1-nano\"           # primary conversational LLM\n",
    "EMB_MODEL = \"text-embedding-3-small\" # lightweight, cost-efficient embeddings\n",
    "\n",
    "# --- Clients ------------------------------------------------------------------\n",
    "# Using the native OpenAI client; authentication sourced from the environment variable declared above.\n",
    "llm_client = OpenAI(api_key=OPENAI_API)\n",
    "\n",
    "# Using Chroma Cloud as the vector store; authentication and multitenancy details supplied via environment variables.\n",
    "# Providing tenant and database explicitly to avoid ambiguous resolution on the backend.\n",
    "chroma = chromadb.CloudClient(\n",
    "    tenant=os.getenv(\"CHROMA_TENANT\"),\n",
    "    database=\"Test\",\n",
    "    api_key=os.getenv(\"CHROMADB_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c732f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generation parameters ----------------------------------------------------\n",
    "# Using explicit decoding hyperparameters to ensure determinism across runs.\n",
    "# TEMPERATURE controls stochasticity during decoding; MAX_TOKENS bounds generation length and cost.\n",
    "TEMPERATUER = 0.7\n",
    "MAX_TOKENS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22098e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding wrapper --------------------------------------------------------\n",
    "# Defining an adapter class to integrate OpenAI’s embedding API with Chroma ingestion.\n",
    "# The class conforms to Chroma’s EmbeddingFunction protocol, enabling drop-in substitution of\n",
    "# embedding providers while preserving a stable interface for downstream vectorization.\n",
    "class OpenAIEmbeddingFunction(EmbeddingFunction[str]):\n",
    "    def __init__(self, client: OpenAI, model: str):\n",
    "        # Storing the OpenAI client instance, responsible for routing requests to the embedding endpoint.\n",
    "        # Storing the specific model identifier that determines the embedding representation.\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # Normalizing the input to a list structure, ensuring batch consistency for both single and multi-string inputs.\n",
    "        if isinstance(inputs, str):\n",
    "            inputs = [inputs]\n",
    "        # Requesting embeddings from the OpenAI API for the normalized input batch.\n",
    "        resp = self.client.embeddings.create(model=self.model, input=inputs)\n",
    "        # Casting returned embeddings into NumPy arrays for numerical stability and compatibility\n",
    "        # with Chroma’s storage and similarity search operations.\n",
    "        return [np.array(item.embedding, dtype=np.float32) for item in resp.data]\n",
    "\n",
    "# Instantiating the embedding function, making it available for explicit precomputation of vectors.\n",
    "emb_fn = OpenAIEmbeddingFunction(llm_client, EMB_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd654dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LLM interaction ----------------------------------------------------------\n",
    "# Using a minimal chat completion request to validate connectivity and authentication.\n",
    "# The interaction specifies:\n",
    "# - model: the target conversational model.\n",
    "# - temperature: the stochasticity of the decoding process.\n",
    "# - max_tokens: the cap on generated tokens to control cost and verbosity.\n",
    "# - messages: a structured dialogue context containing both system and user roles.\n",
    "chat = llm_client.chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=TEMPERATUER,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Eres un asistente útil y conciso.\"},\n",
    "        {\"role\": \"user\", \"content\": \"¿Cuál es la capital de españa?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fbe926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de España es Madrid.\n"
     ]
    }
   ],
   "source": [
    "# --- Validation cell: LLM connectivity test -----------------------------------\n",
    "# Using a direct print of the assistant's reply to confirm that the model responds as expected.\n",
    "print(chat.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
